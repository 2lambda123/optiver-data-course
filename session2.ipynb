{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------\n",
    "<a id='top'></a>\n",
    "## Session contents\n",
    "### [4. Setting data](#setting_data)\n",
    "### [5. Numerical operations and aggregations](#numerical)\n",
    "### [6. Cleaning and filtering data](#cleaning_and_filtering)\n",
    "### [Exercise set 2](#exercises2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------\n",
    "<a id='setting_data'></a>\n",
    "## 4. Setting data\n",
    "\n",
    "When working with a data set it is useful to be able to append data, modify existing data, or create new data that is derived in some way from existing data. We can do this by using the indexing operations from the previous session, as well as some new pandas functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key methods covered:**\n",
    "\n",
    "    pd.concat() - concatenates a list of Series or DataFrame objects together\n",
    "    df.append() - like pd.concat() but as a method on a DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<span style=\"color:green\">Additional resources</span>\n",
    "\n",
    "http://pandas.pydata.org/pandas-docs/stable/10min.html#setting\n",
    "\n",
    "http://pandas.pydata.org/pandas-docs/stable/merging.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll try some basic operations on a DataFrame that contains a few different data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    index=['First', 'Second', 'Third', 'Fourth', 'Fifth'],\n",
    "    data={'A': range(1, 6),\n",
    "          'B': np.random.random(5),\n",
    "          'C': list('UVWXY')})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the datatypes of each column before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a simple example of derived values, try multiplying the entire DataFrame by two - is the result what you expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try dividing the DataFrame by two and take note of the error message. Which lines of the error message are most valuable in debugging your code?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try making a new column 'D' that contains the natural logarithm of column 'A'. You will need to use the numpy library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#solutions\n",
    "%load solutions/setting_sol1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data can be overwritten by using loc - try overwriting row 0, column A with the value $\\pi$ and view the DataFrame again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#solutions\n",
    "%load solutions/setting_sol2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now check the data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should notice that column A is now a float64 rather than an int64 type. Numpy and pandas automatically convert your columns to the most general data type that your data shares - in this case, a floating point number in the first value resulted in __all__ values being floating point numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using __.loc__ with a new index value appends new rows and/or columns as necessary (called \"setting with enlargement\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['Sixth'] = [6, np.random.random(), 'Z', np.log(6)]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['Seventh', 'E'] = dt.datetime.now()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an alternative to loc, we can use the append method to add new rows to a DataFrame (where the 'other' argument is a dict/Series or another DataFrame)\n",
    "\n",
    "    df = df.append(other)\n",
    "    \n",
    "Note that, unlike a list's append method which is an inplace operation, a DataFrame's append method returns a new DataFrame object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try append now by creating a new Series object and appending it to df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have multiple Series or DataFrame objects to concatenate, we can avoid multiple append statements by using a single concat statement\n",
    "\n",
    "    df = pd.concat([df1, df2, df3])\n",
    "    \n",
    "This function has many more options than append, but we'll skip over these for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try using concat in the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------\n",
    "<a id='numerical'></a>\n",
    "## 5. Numerical operations and aggregations\n",
    "\n",
    "Since pandas objects are constructed from numpy arrays, we can use numerical functions from the numpy package on our Series and DataFrame objects. It is conventional to import numpy in the following way\n",
    "\n",
    "    import numpy as np\n",
    "    \n",
    "Pandas objects also have a variety of useful numerical and statistical methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Key methods covered: **\n",
    "    \n",
    "    np.where()\n",
    "    df.where()\n",
    "    df.mean(), df.min(), df.max(), etc.\n",
    "    df.diff(), df.sum()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<span style=\"color:green\">Additional resources</span>\n",
    "\n",
    "http://pandas.pydata.org/pandas-docs/stable/10min.html#operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's return to our DataFrame from earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(index=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday'],\n",
    "    data={'optiver_turnover': [ 46386,  43775,  75742,  17474, np.nan],\n",
    "          'total_turnover':   [278837, 439771, 583722, 358834, np.nan],\n",
    "          'volatility':       [  12.5,   14.0,   21.5,   16.0,   17.0]})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try adding Optiver's market share (optiver_turnover divided by total_turnover as a percentage) as a column in df called 'market_share':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#solutions\n",
    "%load solutions/numerical_sol1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost all numpy functions can be applied directly to a Series or DataFrame. For example, let's round the market share percentage to two decimal places using __np.round__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['market_share'] = np.round(df['market_share'], 2)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrames have a number of basic statistical methods, such as __mean()__, __min()__, __max()__, __std()__, and __quantile(q)__ (where q=0.5 is the median). By default they are applied column-by-column and ignore missing data. Try a few of them in the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They can also be applied row-by-row, by adding the argument axis=1. Again, try a few of these below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrames also have a __describe__ method that calculates a number of summary statistics simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as statistical methods, basic aggregation methods are also available. Try calculating the weekly optiver turnover and total turnover using __sum__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#solutions\n",
    "%load solutions/numerical_sol2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other useful methods include __diff__, __cumsum__, and __rank__. Try these below and interpret the meaning of the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison operators (<, <=, >, >=, ==, !=) will turn out to be very useful when filtering DataFrames. As an example, let's create a column called 'good_market_share' which is True when market share exceeded 10% and false otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#solutions\n",
    "%load solutions/numerical_sol3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting data type is Boolean (True or False). Any comparison with missing data returns False. One interesting way to use Boolean columns is to apply the sum operator - True and False are converted to 1 and 0 respectively, so the result is the number of times the condition was true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Number of days with good market share:', df['good_market_share'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try making a column called 'high_volatility' which is True when volatility is greater than 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#solutions\n",
    "%load solutions/numerical_sol4.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine Boolean arrays together with all the usual logical operators.\n",
    "\n",
    "    low_volatility = ~ df['high_volatility']  # NOT operator\n",
    "    positive_and_even =             (df.A > 0) & (mod(df.A, 2) == 0)  # AND operator (note: the brackets around the conditional statements are necessary)\n",
    "    positive_or_even =              (df.A > 0) | (mod(df.A, 2) == 0)  # OR operator (not exclusive-or/XOR)\n",
    "    positive_or_even_but_not_both = (df.A > 0) ^ (mod(df.A, 2) == 0)  # XOR operator\n",
    "    \n",
    "Try creating a column which is True when we have high volatility and good market share."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#solutions\n",
    "%load solutions/numerical_sol5.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, __np.where__ provides a useful way of mapping True/False values to other values. See if you can make a 'commentary' column which has the value 'LowVol' if volatility is low and 'HighVol' if volatility is high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#solutions\n",
    "%load solutions/numerical_sol6.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataFrame method __df.where__ is similar to np.where, but can only modify values where the condition is False."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "<a id='cleaning_and_filtering'></a>\n",
    "## 6. Cleaning and filtering data\n",
    "\n",
    "Cleaning data prior to analysis is one of the most essential steps in ensuring the outputs of our analysis are accurate and can be relied upon.\n",
    "\n",
    "For example, during the auction period the bid and ask prices can often take unrealistic values. It might even be a good idea for some projects to remove the volatile auction period altogether. We can use some techniques to remove certain blocks of data from our DataFrame.\n",
    "\n",
    "The easiest way to do this is to apply a Boolean mask. For example, the line of code below would only return the subset of the original DataFrame where column1 was positive.\n",
    "\n",
    "    df[df[column1] > 0]\n",
    "\n",
    "It works by applying your Boolean criteria to column1 which yields a list of True and False values. This is then used to determine which rows of the DataFrame are retained (keep True, discard False).\n",
    "Alternatively, you may wish to remove segments of your data (for example, to avoid the opening and closing auctions). The code below shows how Boolean masks can be applied to the index of a DataFrame too.\n",
    "\n",
    "    df[df.index.hour > 9] \n",
    "\n",
    "The key methods are __df.isnull(), df.dropna(), df.fillna()__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<span style=\"color:green\">YouTube video</span>\n",
    "\n",
    "Watch the video below until the 2 hour and 5 minute mark.\n",
    "\n",
    "\n",
    "https://www.youtube.com/watch?v=6ohWS7J1hVA&start=6661"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key methods covered:**\n",
    "\n",
    "    df.drop - drops certain rows/columns\n",
    "    masking - returning a subsection of the DataFrame according to certain criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Refer to the Python Data Science Handbook below for more information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<span style=\"color:green\">Additional resources</span>\n",
    "\n",
    "http://pandas.pydata.org/pandas-docs/stable/indexing.html#boolean-indexing\n",
    "http://nbviewer.jupyter.org/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.04-Missing-Values.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "If you have just started here or would like to refresh your df_qte object, run the line below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load solutions/cleaning_start.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far the df_qte DataFrame appears to be behaving as it should - but are there any hidden gotchas?\n",
    "\n",
    "A good way to check this is to visualise the data. Pandas has some basic built in plotting functionality which allows us to plot DataFrame or Series objects. First run \n",
    ">%matplotlib inline \n",
    "\n",
    "to ensure figures display inline rather than as pop-up windows, and then try to __.plot()__ the bid and ask prices. (see: http://pandas.pydata.org/pandas-docs/stable/visualization.html)\n",
    "\n",
    "What do you notice? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qte[['BidPrice', 'AskPrice']].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's quite clear from the above charts that we've got some 0 values in our data. Check for yourself by using a mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#solutions\n",
    "%load solutions/clean_sol1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It would probably be a good idea (in most cases) to filter out these entries with a value 0 or less. We can do this by replacing the DataFrame with a subset of itself where bid and ask prices are positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#solutions\n",
    "%load solutions/clean_sol2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we revisit the charts of bid and ask prices, we should find the numbers much more reasonable now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qte.BidPrice.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qte.AskPrice.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since masks are simply lists of Boolean operators, we should be able to apply **multiple masks** to a DataFrame with ease. For example, if we have 2 criteria - A and B, and A returns a Series of \n",
    "\n",
    "    [True, False] \n",
    "    \n",
    "and B returns a Series of \n",
    "\n",
    "    [False, False]\n",
    "    \n",
    "applying both masks A and B will simply return \n",
    "\n",
    "    [False, False]\n",
    "\n",
    "Remember, when we combine Boolean operators only 2 Trues will return a True.\n",
    "\n",
    "Let's try and apply 2 masks to our df_qte DataFrame - **BidSize >= 30 and BidSize <= 40**. Recall the format of masks!\n",
    "\n",
    "    mask_name = df['column'] *criterion*\n",
    "    \n",
    "    df[mask_1 & mask_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing data may be represented in pandas using either Python's None or numpy's np.nan (rendered as NaN when displayed). We may encounter missing data if there are data quality issues, or we if we choose to set some (wrong or uninteresting) data as missing.\n",
    "\n",
    "There are a few options for dealing with missing data:\n",
    "\n",
    "Set to another number:\n",
    "\n",
    "    - A fixed value (.fillna)\n",
    "    - Fill forwards from the previous non-NaN value (.ffill)\n",
    "    - Fill backwards from the next non-NaN value (.bfill)\n",
    "    \n",
    "Remove rows or columns:\n",
    "\n",
    "    df.dropna(how='any', axis=0)  # drop row if any of its values are NaN\n",
    "    df.dropna(how='any', axis=1)  # drop column if any of its values are NaN\n",
    "    df.dropna(how='all')  # drop row if all values are NaN\n",
    "    df.dropna(thresh=2)  # keep row if 2 or more entries are not NaN\n",
    "    \n",
    "Forward-fill is usually better to use than back-fill for trading data, since the current (missing) value is likely to be the most recent non-NaN value.\n",
    "\n",
    "If you need to do more complicated operations with NaNs, there are methods that return True or False if the data is or is not NaN.\n",
    "\n",
    "    df.isnull()  # or df.isna() - returns True if value is NaN and False otherwise\n",
    "    df.notnull()  # or df.notna() - returns False if value is NaN and True otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try dealing with zero-priced bids/offers with missing-data operations instead. We'll reload the data again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load solutions/cleaning_start.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, set invalid bids and offers to np.nan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, forward fill the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then drop any remaining NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, check the plots again to make sure our bids and offers are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------\n",
    "<a id='exercises2'></a>\n",
    "## Exercise set 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Using the cleaned df_qte dataframe, add the bid-ask midpoint as a column.\n",
    "2. Calculate the following quantities.\n",
    "    - The average bid-ask spread in the product.\n",
    "    - The open/low/high/close of the midpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
