{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------\n",
    "<a id='top'></a>\n",
    "## Session contents\n",
    "### [10. Working with Timestamps](#timestamps)\n",
    "### [11. Resampling data](#resampling_data)\n",
    "### [12.  Merging by time](#merge_time)\n",
    "### [Exercise set 4](#exercises4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------\n",
    "<a id='timestamps'></a>\n",
    "## 10. Working with Timestamps\n",
    "\n",
    "Since financial data are often time series, it makes sense to set the index of our DataFrame to be the timestamp of each event. Pandas provides a __pd.to_datetime()__ function to convert strings or other date objects into datetime64 objects that pandas likes to work with.\n",
    "\n",
    "When a column of timestamps (datetime64 objects) is set as the index, a lot of time-series methods for the DataFrame become available.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<span style=\"color:green\">Additional resources</span>\n",
    "\n",
    "http://pandas.pydata.org/pandas-docs/stable/timeseries.html\n",
    "http://nbviewer.jupyter.org/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.11-Working-with-Time-Series.ipynb#Dates-and-times-in-pandas:-best-of-both-worlds\n",
    "\n",
    "http://pandas.pydata.org/pandas-docs/version/0.19.0/generated/pandas.DataFrame.asof.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "If you have just started here or would like to refresh your df_qte object, run the line below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qte = pd.read_csv('data/quote.csv')\n",
    "df_qte.columns = ['Index', 'Time', 'BidPrice', 'BidSize', 'AskPrice', 'AskSize']\n",
    "df_qte.drop(columns='Index', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qte.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to replace the index of our DataFrame with the values in the column 'Time', converting these values into datetime objects in the process. \n",
    "\n",
    "Check for yourself what type of an object the values in column 'Time' are currently stored as."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Were we to set the Time column as the index right now, pandas would not recognise it as an index of timestamps.\n",
    "\n",
    "Let's try out the pd.to_datetime function on the first Time value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function works for a variety of time objects and formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime(dt.datetime(2017, 5, 5, 12, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime('2017-05-05 12:30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime(1493987400000000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The latter format is called Unix time or Unix epoch and is the standard high-precision timestamp format in our trading systems. It is defined as the number of seconds (or milli/micro/nanoseconds) since midnight 1 January 1970 (1/1/1970 00:00:00 GMT)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can set Time as a DateTimeIndex by first converting the column's values to pandas Timestamp objects, then setting it as the index in the usual way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qte['Time'] = pd.to_datetime(df_qte['Time'])\n",
    "df_qte = df_qte.set_index('Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qte.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the index of the DataFrame is now a DatetimeIndex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The DateTimeIndex is a lot more flexible than a regular index in how data can be selected. We can use loc with a datetime object to return a row at that exact time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qte.loc[dt.datetime(2014, 1, 2, 1, 8, 22, 692413)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use loc with a datetime string to return all rows that match the specified datetime, up to the level of detail provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qte.loc['2014-01-02 00:45']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slicing with datetime strings allows for selection of a specified time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qte.loc['2014-01-02 00:00':'2014-01-02 01:14']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To select the same time period across multiple days, we can use the between_time method. This can be useful for separating out morning and afternoon trading sessions, or filtering out the auction period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_qte.between_time('00:00', '01:15')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the df.asof() method takes a timestamp and returns the most recent non-NaN row. Try finding the last quote prices as at 01:15 with this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "### Working with timezones - OneTick\n",
    "\n",
    "Depending on the timezone parameter selected in your otq query, your timestamps may be imported as a tz-aware object. This is an object that belongs to the datetime (dt) package. \n",
    "\n",
    "If you would like to convert a tz-aware index into a tz-na√Øve index, see the following Stackoverflow post. This is especially relevant when you are pulling data from different databases in OneTick.\n",
    "\n",
    "http://stackoverflow.com/questions/16628819/convert-pandas-timezone-aware-datetimeindex-to-naive-timestamp-but-in-certain-t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "<a id='resampling_data'></a>\n",
    "## 11. Resampling data\n",
    "\n",
    "The main method used to resample data is __df.resample__, which is available when the DataFrame/Series object has a valid DatetimeIndex.\n",
    "\n",
    "It is common to combine resampling with __dropna()__ or __fillna()__ methods as resampling upwards (higher frequency than the original data) will create null values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<span style=\"color:green\">YouTube video</span>\n",
    "\n",
    "Watch the following video until around the 2 hour 13 minute mark.\n",
    "\n",
    "https://www.youtube.com/watch?v=JNfxr4BQrLk&start=6956"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key methods covered:**\n",
    "\n",
    "    df.resample - similar to .asfreq\n",
    "    df.fillna - fills NaNs according to specified logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<span style=\"color:green\">Additional resources</span>\n",
    "\n",
    "http://nbviewer.jupyter.org/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.11-Working-with-Time-Series.ipynb#Resampling,-Shifting,-and-Windowing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "If you have just started here or would like to refresh your df_qte object, run the line below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qte = pd.read_csv('data/quote.csv')\n",
    "df_qte.columns = ['Index', 'Time', 'BidPrice', 'BidSize', 'AskPrice', 'AskSize']\n",
    "df_qte.drop(columns='Index', inplace=True)\n",
    "df_qte['Time'] = pd.to_datetime(df_qte['Time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's try resampling the data into 5 minute buckets using the df.resample() method.\n",
    "\n",
    "http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.resample.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we simply call .resample with a frequency (e.g. '100ms', '1s', '5min', '2h', '1D'), we will end up with a DatetimeIndexResampler object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qte = df_qte.set_index('Time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is because we have yet to specify how we want to resample the data. Try applying a __.last(), .mean()__, or __.sum()__ operation after .resample. In what situations would each be the appropriate operation to use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_qte.resample('20min').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resampling often results in NaNs. This happens when no data comes through within that sample period. \n",
    "\n",
    "It is appropriate to forward fill the NaNs if the data represents updates to the state of some object (e.g. an order book, a volatility curve, an autotrader's parameters). If the data represents individual events like trade ticks, then forward filling will overcount trade volumes. In this case, it is more appropriate to fill NaNs with 0, to drop the NaNs, or to simply keep them in the DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes our data consists of distinct groups and we want to apply a resampling operation - for instance, resample all relative expiries of a volatility surface. Let's load up that data again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vols = pd.read_csv('data/vols.csv')\n",
    "df_vols.TIMESTAMP = pd.to_datetime(df_vols.TIMESTAMP)\n",
    "df_vols = df_vols.set_index('TIMESTAMP', drop=True)\n",
    "df_vols.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try resampling and getting the first update every 10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This operation isn't what we wanted, since we get a relative expiry at random for each time bucket. We can instead perform a resample after a __groupby__ before getting the first entry, so that we resample each relative expiry properly.\n",
    "\n",
    "Try getting the volatility values for each relative expiry every 10 mins by:\n",
    "1. Grouping by RELATIVE_EXPIRY\n",
    "2. Resampling\n",
    "3. Selecting only the VOLATILITY column\n",
    "4. Unstacking RELATIVE_EXPIRY from the index to the columns.\n",
    "5. Forward-filling NaNs, if any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#solutions\n",
    "%load solutions/resample_sol1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "<a id='merge_time'></a>\n",
    "# 12. Merging by time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The joins we covered in the previous session were __exact__ joins. That is, the data were only joined together if the values of each key were identical.\n",
    "\n",
    "If we want to join two time series together, we'll find that the timestamps rarely match exactly. The join we are after is usually along the lines of \"for each row in [left source], give me the most recent data from [right source]\".\n",
    "\n",
    "The classic example in trading data is joining trade and quote data together. We want to know, for each trade, what the state of the order book was at that time.\n",
    "\n",
    "Pandas implements this kind of join with the function\n",
    "\n",
    "    pd.merge_asof()\n",
    "    \n",
    "which is very similar to pd.merge. There a few additional arguments that are specific to pd.merge_asof:\n",
    "\n",
    "    direction - whether to find the matching row from the right source, either 'backward' (default), 'forward' (i.e. next row), or 'nearest'\n",
    "    tolerance - only match if the difference between indexes is below this number, e.g. tolerance=pd.Timedelta('1s') will only join the right source if its time was within 1 second of the left source\n",
    "    by - do the time join for each value in these columns separately. E.g. by='FEEDCODE' will perform the join for each feedcode separately. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<span style=\"color:green\">Additional resources</span>\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/generated/pandas.merge_asof.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load up both quote and trade data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qte = pd.read_csv('data/quote.csv')\n",
    "df_qte.columns = ['Index', 'Time', 'BidPrice', 'BidSize', 'AskPrice', 'AskSize']\n",
    "df_qte.Time = pd.to_datetime(df_qte.Time)\n",
    "df_qte.drop(columns='Index', inplace=True)\n",
    "df_qte = df_qte.set_index('Time', drop=True)\n",
    "df_qte.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trd = pd.read_csv('data/trade.csv')\n",
    "df_trd.TIMESTAMP = pd.to_datetime(df_trd.TIMESTAMP)\n",
    "df_trd = df_trd.set_index('TIMESTAMP', drop=True)\n",
    "df_trd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each trade, we can join on the most recent quote with pd.merge. Since our join key, the timestamps, are in each DataFrame's index, we have to set left_index=True and right_index=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge_asof(df_trd, df_qte, left_index=True, right_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like our trade prices don't always match up with the best bid or offer, i.e. there are synchronisation issues. Try to calculate the percentage of trades that don't line up with our quote data, using conditional expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#solutions\n",
    "%load solutions/time_sol1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "<a id='exercises4'></a>\n",
    "# Exercise set 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the 30-minute price change for each trade, and plot the price changes as a time series and a histogram. Your 30-minute delayed price should be the midpoint of the nearest quote update. (Hint: Use __pd.merge_asof__, __df.tshift__, and __df.plot.hist__.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
